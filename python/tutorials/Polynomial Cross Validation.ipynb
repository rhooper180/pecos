{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "Cross validation can be used to estimte the error in an approximation. Following Hastie, let $\\hat{f}$ denote an approximation of\n",
    "a function $f$ built using training data \n",
    "\n",
    "$$\\mathcal{X}_{train}=\\{X_{train},Y_{train}=f(X_{train})\\}$$\n",
    "\n",
    "and let $L(Y,\\hat{f}(X))$ denote a loss function use to measure errors in the approximation. Here we will use\n",
    "\n",
    "$$L(Y,\\hat{f}(X))=(Y-\\hat{f}(X))^2$$\n",
    "\n",
    "Ideally we would like to compute the prediction error over an independent test data $\\mathcal{X}_{test}$, i.e.\n",
    "\n",
    "$$Err_\\mathcal{X}=\\operatorname{E}_{\\mathcal{X}_{test}}[L(f(\\mathcal{X}_{test}),\n",
    "\\hat{f}(\\mathcal{X}_{test},p)\\mid\\mathcal{X_{train}})].$$\n",
    "\n",
    "which is the error for the specific training set $\\mathcal{X}_train$.\n",
    "When evaluating $f$ is expensive, however, computing an independent test set may not be possible. Instead\n",
    "of using function evaluations for approximating error we could use them to futher improve the accuracy of \n",
    "the approximation. However if we do this we can not longer compute $Err_\\mathcal{X}$.\n",
    "\n",
    "Cross validation can be used to approximate the related quantity, known as the expected prediction error. \n",
    "This quantity averages over everything that is random, i.e. the training and test data.\n",
    "\n",
    "$$Err=\\operatorname{E}_\\mathcal{X_{train}}[Err_\\mathcal{X}]$$\n",
    "\n",
    "The training error, which is the loss computed the training data\n",
    "\n",
    "$$\\bar{err}=\\frac{1}{M}\\sum_{i=1}^M L(y_i,\\hat{f}(x_i))$$ is incorrectly used to estimate prediction error.\n",
    "Training error decrease with model complexity. If we contsruct an interpolant of $f$ the training error will be zero\n",
    "by construction, however the model will likely overfit the training data and will under estimate the prediction error.\n",
    "\n",
    "The cross validation can be used to estimate the prediction error when gnerating an independent test set is not \n",
    "feasiable. K-folds cross validation uses all the available training data to construct the approximation. \n",
    "Speficially the data is split into K roughly sized sets. The partition is constructed randomly without replacement.\n",
    "K-1 sets are used to train the approximation and the remaining\n",
    "data is used to estimate the prediction error. This procedure is repeated for K times, starting by holding out 1st data set,\n",
    "then holding out the second data set and so on. Once complete the K estimates of the \n",
    "prediction error are combined. \n",
    "\n",
    "Let $\\hat{f}^{-k}$ denote the approximation trained with the kth data set removed\n",
    "then the cross validation error is given by\n",
    "\n",
    "$$CV(\\hat{f})=\\frac{1}{M}\\sum_{i=1}^M L(y_i,\\hat{f}^{-\\kappa(i)}(x_i)).$$\n",
    "\n",
    "Here $\\kappa: \\{1,\\ldots,M\\}\\rightarrow \\{1,\\ldots,K\\}$ denotes an indexing function that return the \n",
    "number of the partition to which the ith data obervation belongs.\n",
    "\n",
    "Asymptotically cross validation is an unbiased estimate of the expected prediction error $Err$. \n",
    "The bias decreases with as the number of samples $M$ increases\n",
    "Leave one out cross validation, $K=M$ and $\\kappa(i)=i$ has the smallest bias for a fixed $M$, however the\n",
    "variance can be large. For a given data set $\\mathcal{X}_{train}$ each of the training sets will be very similar. \n",
    "Thus the estimate of $CV$ will be very dependent on $\\mathcal{X}_{train}$ thus producing a lot of variability when \n",
    "$\\mathcal{X}_{train}$ is varied. Setting $K=10$ will result in a smaller variance, but a larger bias for small \n",
    "number of samples $M$. Five- or tenfold cross-validation can be a good compromise between bias and variance \n",
    "-see Breiman and Spector (1992) and Kohavi (1995).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize plotting environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['text.usetex'] = True # use latex for all text handling\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['xtick.labelsize'] = 20\n",
    "mpl.rcParams['ytick.labelsize'] = 20\n",
    "mpl.rcParams['text.latex.preamble'] = [r'\\usepackage{siunitx}', r'\\usepackage{amsmath}']\n",
    "\n",
    "# to just create static png plots use \n",
    "%matplotlib inline\n",
    "\n",
    "# to allow interactive plots use \n",
    "# %matplotlib notebook\n",
    "\n",
    "# use jupyter nbconvert --to python <notebook_name.ipynb> to convert to python script\n",
    "\n",
    "# if having trouble with errors like cannot import _backports go to Kernel toolbar\n",
    "# and select Restart & Clear Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function we want to approximate\n",
    "We will use the oscillstory frunction from the Genz integration test suite.\n",
    "$$f(x)=cos\\left(2\\pi w_1+\\sum_{i=1}^d c_i x_i\\right),\\quad x\\in[0,1]^d$$\n",
    "where we set $$w_1=0, \\quad c_i=\\frac{i-0.5}{d}K, \\quad K=C\\sum_{i=1}^d c_i, \\quad C=10$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PyDakota.regression import *\n",
    "from PyDakota.models.genz import GenzFunction\n",
    "from PyDakota.approximation import *\n",
    "from PyDakota.math_tools import compute_hyperbolic_indices\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from scipy.misc import comb\n",
    "from functools import partial\n",
    "#import numpy\n",
    "\n",
    "numpy.random.seed(3)\n",
    "\n",
    "num_vars = 2\n",
    "variables = BoundedVariables()\n",
    "ranges = define_homogeneous_ranges(num_vars, 0., 1.);\n",
    "variables.set_ranges(ranges)\n",
    "\n",
    "function = GenzFunction('oscillatory',num_vars)\n",
    "function.set_coefficients(10., 'no-decay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the approximation\n",
    "Dakota supports many approximation methods. Here we focus on how to build a total-degree multivariate polynomial approximation\n",
    "\n",
    "$$f(x)\\approx p(T(x))=p(u)=\\sum_{\\|\\lambda\\|_1\\le 10} c_\\lambda\\phi_\\lambda(u).$$ \n",
    "\n",
    "To define a 3rd order total degree monomial approximation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variable transformation T\n",
    "var_transform = AffineVariableTransformation()\n",
    "var_transform.set_variables(variables)\n",
    "\n",
    "approx = Monomial()\n",
    "approx.set_variable_transformation(var_transform)\n",
    "\n",
    "degree = 10\n",
    "basis_indices = compute_hyperbolic_indices(num_vars, degree, 1.)\n",
    "approx.set_basis_indices(basis_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the cross validation\n",
    "PyDakota provides a object to solve linear systems using cross validation. \n",
    "We want to repeat cross validation many times so lets create some functions to help us do this. The function\n",
    "cross_validated_solve returns the cross validation error $CV$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validated_solve(basis_matrix, training_function_vals,\n",
    "                          regression_type, regression_opts, seed):\n",
    "    regression_opts['cv-opts']['seed']=seed\n",
    "    cv_solver = CrossValidatedSolver()\n",
    "    cv_solver.set_linear_system_solver(regression_type)\n",
    "    cv_solver.solve(basis_matrix, training_function_vals, regression_opts)\n",
    "    scores = cv_solver.get_best_scores()\n",
    "    solutions = cv_solver.get_final_solutions()\n",
    "    return scores, solutions\n",
    "\n",
    "def cross_validated_solve_helper(args):\n",
    "    return cross_validated_solve(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets perform cross validation for many different training sets. We will use Python's multiprocessing.Pool function\n",
    "to do the cross validation on each training data independently. Note we are not computing the approximation on each fold,\n",
    "for a given data set, indendently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that_took: 3.53456020355\n",
      "that_took: 0.261016845703\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "\n",
    "num_training_samples = 2*basis_indices.shape[1]\n",
    "num_folds = 10;\n",
    "regression_type=SVD_LEAST_SQ_REGRESSION\n",
    "function_opts = {'eval_type':'value-grad'}\n",
    "cv_opts = {'num-points':num_training_samples,'num-folds':num_folds}\n",
    "regression_opts = {'verbosity':0,'cv-opts':cv_opts,'store-history':True}\n",
    "\n",
    "\n",
    "num_trials = 1000;\n",
    "t0 = time.time()\n",
    "max_eval_concurrency=10\n",
    "# Using max_eval_concurrency>1 is slower than using max_eval_concurrency=1\n",
    "# I think this has something to do with OMP_NUM_THREADS used by NumPy\n",
    "# if want to use\n",
    "# max_eval_concurrency=max(multiprocessing.cpu_count(),1)\n",
    "# open tutorial with OMP_NUM_THREADS=1 jupyter notebook Polynomial Cross Validation.ipynb\n",
    "\n",
    "pool = Pool(max_eval_concurrency)\n",
    "args = []\n",
    "seeds = numpy.random.permutation(numpy.arange(1,10*num_trials))[:num_trials]\n",
    "for i in range(num_trials):\n",
    "    training_samples = numpy.random.uniform(0,1,(num_vars,num_training_samples))\n",
    "    training_function_vals = function.value(training_samples,function_opts)\n",
    "    basis_matrix = approx.generate_basis_matrix(training_samples)\n",
    "    args.append([basis_matrix,training_function_vals,regression_type,regression_opts,seeds[i]])\n",
    "print 'that_took:', time.time()-t0\n",
    "t0 = time.time()\n",
    "result = numpy.asarray(pool.map(cross_validated_solve_helper,args))\n",
    "print 'that_took:', time.time()-t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare the cross validation error with the RMSE error for each training set. We do this by \n",
    "evaluting the approximations, computed for each training set, on a set of validation data not used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that_took: 9.25084090233\n"
     ]
    }
   ],
   "source": [
    "num_validation_samples = 1000\n",
    "validation_samples = numpy.random.uniform(0,1,(num_vars,num_validation_samples))\n",
    "validation_vals = function.value(validation_samples,function_opts)\n",
    "\n",
    "t0 = time.time()    \n",
    "mse = []\n",
    "cv_scores = []\n",
    "for i in range(num_trials):\n",
    "    cv_scores_i, solutions = result[i]\n",
    "    cv_scores.append(cv_scores_i)\n",
    "    approx.set_coefficients(solutions)\n",
    "    approx_validation_vals = approx.value(validation_samples)\n",
    "    mse.append(numpy.linalg.norm(approx_validation_vals-validation_vals,axis=0)**2/num_validation_samples)\n",
    "mse = numpy.asarray(mse)\n",
    "cv_scores = numpy.asarray(cv_scores)\n",
    "print 'that_took:', time.time()-t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the cross validation error $CV$ vs the $Err_\\mathcal{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6cAAAFwCAYAAABEuA3OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIB0wikAAADphFMAAADSCacAAACk+x/qcagpgUdw4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faab9059650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_bins=100\n",
    "num_rhs = mse[0].shape[0]\n",
    "f,axs=plt.subplots(1,num_rhs,sharey=True,figsize=(16, 6))\n",
    "axs = axs.ravel()\n",
    "for j in range(num_rhs):\n",
    "    bins=numpy.logspace(numpy.log10(mse[:,j].min()),numpy.log10(mse[:,j].max()), num_bins)\n",
    "    n, bins, patches = axs[j].hist(mse[:,j], bins=bins, normed=None, facecolor='green', alpha=0.5,label='Err')\n",
    "    bins=numpy.logspace(numpy.log10(cv_scores[:,j].min()),numpy.log10(cv_scores[:,j].max()), num_bins)\n",
    "    n, bins, patches = axs[j].hist(cv_scores[:,j], bins=bins, normed=None, facecolor='blue', alpha=0.75,label='CV score')\n",
    "    axs[j].set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
