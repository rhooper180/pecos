{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyDakota.regression import *\n",
    "from PyDakota.models.genz import GenzFunction\n",
    "from PyDakota.approximation import *\n",
    "from PyDakota.math_tools import compute_hyperbolic_indices\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from scipy.misc import comb\n",
    "from functools import partial\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(3)\n",
    "\n",
    "num_vars = 2\n",
    "variables = BoundedVariables()\n",
    "ranges = define_homogeneous_ranges(num_vars, 0., 1.);\n",
    "variables.set_ranges(ranges)\n",
    "\n",
    "function = GenzFunction('oscillatory',num_vars)\n",
    "function.set_coefficients(10., 'no-decay')\n",
    "\n",
    "var_transform = AffineVariableTransformation()\n",
    "var_transform.set_variables(variables)\n",
    "\n",
    "degree = 20\n",
    "approx = Monomial()\n",
    "approx.set_variable_transformation(var_transform)\n",
    "basis_indices = compute_hyperbolic_indices(num_vars, degree, 1.)\n",
    "approx.set_basis_indices(basis_indices)\n",
    "\n",
    "num_training_samples = 100#2*basis_indices.shape[1]\n",
    "training_samples = numpy.random.uniform(0,1,(num_vars,num_training_samples))\n",
    "function_opts = {'eval_type':'value-grad'}\n",
    "training_function_vals = function.value(training_samples,function_opts)\n",
    "basis_matrix = approx.generate_basis_matrix(training_samples)\n",
    "\n",
    "num_folds = 10; seed=-1# deterministic partition\n",
    "regression_type = LEAST_ANGLE_REGRESSION#LASSO_REGRESSION#ORTHOG_MATCH_PURSUIT#\n",
    "cv_opts = {'num-points':num_training_samples,'num-folds':num_folds,'seed':seed}\n",
    "regression_opts = {'verbosity':0,'cv-opts':cv_opts,'store-history':True}\n",
    "# setting store_history to False causes cv_solver to terminate early when generating best solution\n",
    "# there is some numerical drift I need to find the cause of.\n",
    "cv_solver = CrossValidatedSolver()\n",
    "cv_solver.set_linear_system_solver(regression_type)\n",
    "print numpy.linalg.norm(basis_matrix), numpy.linalg.norm(training_function_vals)\n",
    "cv_solver.solve(basis_matrix, training_function_vals, regression_opts)\n",
    "scores = cv_solver.get_best_scores()\n",
    "cv_iterator = cv_solver.get_cross_validation_iterator()\n",
    "cv_iterator = cast_to_linear_system_cross_validation_iterator(cv_iterator)\n",
    "cv_scores = cv_iterator.get_scores()\n",
    "cv_residuals = cv_iterator.get_unique_tolerances()\n",
    "best_tolerances = cv_iterator.get_adjusted_best_residual_tolerances()[:,0]\n",
    "cv_solutions = cv_solver.get_final_solutions()\n",
    "print \"Cross validation error:\",scores\n",
    "print \"Best tolerances:\",best_tolerances\n",
    "print \"Exit tolerance values:\",cv_solver.get_final_residuals()\n",
    "\n",
    "num_rhs = training_function_vals.shape[1]\n",
    "num_validation_samples = 100\n",
    "validation_samples = numpy.random.uniform(0,1,(num_vars,num_validation_samples))\n",
    "validation_function_vals = function.value(validation_samples, function_opts)\n",
    "regression_opts = {'regression_type':regression_type}\n",
    "solver = regression_solver_factory(regression_opts) \n",
    "solver.solve(basis_matrix, training_function_vals, regression_opts)\n",
    "mse = []\n",
    "residuals = []\n",
    "for i in range(num_rhs):\n",
    "    coef = solver.get_solutions_for_all_regularization_params(i)\n",
    "    num_steps = coef.shape[1]\n",
    "    approx.set_coefficients(coef)\n",
    "    approx_validation_vals = approx.value(validation_samples)\n",
    "    approx_training_vals = approx.value(training_samples)\n",
    "    residuals.append(numpy.linalg.norm(numpy.tile(training_function_vals[:,i],(num_steps,1)).T-approx_training_vals,axis=0))\n",
    "    assert numpy.allclose(solver.get_residuals_for_all_regularization_params(i),residuals[i])\n",
    "    #print cv_solver.get_residuals_for_all_regularization_params(i)\n",
    "    mse.append(1./num_validation_samples*numpy.linalg.norm(numpy.tile(validation_function_vals[:,i],(num_steps,1)).T-approx_validation_vals,axis=0)**2)\n",
    "\n",
    "\n",
    "approx.set_coefficients(cv_solutions)\n",
    "cv_approx_validation_vals = approx.value(validation_samples)\n",
    "cv_approx_training_vals = approx.value(training_samples)\n",
    "cv_solution_residuals = numpy.linalg.norm(training_function_vals-cv_approx_training_vals,axis=0)\n",
    "assert numpy.allclose(cv_solver.get_final_residuals(), cv_solution_residuals)\n",
    "cv_approx_mse = 1./num_validation_samples*numpy.linalg.norm(validation_function_vals-cv_approx_validation_vals,axis=0)**2\n",
    "print mse[-1].shape, coef.shape, training_samples.shape, basis_indices.shape, residuals[-1].shape\n",
    "\n",
    "%pylab inline\n",
    "f,axs=pylab.subplots(1,num_rhs,sharey=True,figsize=(16, 6))\n",
    "axs = axs.ravel()\n",
    "for i in range(num_rhs):\n",
    "    axs[i].loglog(cv_residuals[i]/.9,cv_scores[i],'-o',label='CV error')\n",
    "    axs[i].loglog(residuals[i],mse[i],'-s',label='$\\ell_2$ error')\n",
    "    axs[i].axvline(best_tolerances[i],linestyle='--',color='k',label='$\\\\varepsilon$')\n",
    "    axs[i].loglog(cv_solution_residuals[i],cv_approx_mse[i],'*',ms=10,label='CV solution $\\ell_2$ error')\n",
    "    axs[i].set_title('QoI %d'%(i+1))\n",
    "    axs[i].set_xlabel('$\\|Ax-b\\|_2$')\n",
    "axs[-1].legend(fontsize=16,bbox_to_anchor=(1.1, .7))\n",
    "\n",
    "def cross_validated_solve(basis_matrix,regression_type, training_function_vals, regression_opts):\n",
    "    cv_solver = CrossValidatedSolver()\n",
    "    cv_solver.set_linear_system_solver(regression_type)\n",
    "    cv_solver.solve(basis_matrix, training_function_vals, regression_opts)\n",
    "    scores = cv_solver.get_best_scores()\n",
    "    return scores\n",
    "\n",
    "regression_opts = {'verbosity':0,'cv-opts':cv_opts,'store-history':True}\n",
    "cv_solve = partial(cross_validated_solve,regression_type=regression_type,\n",
    "                   training_function_vals=training_function_vals, regression_opts=regression_opts)\n",
    "max_eval_concurrency=4\n",
    "t0 = time.time()\n",
    "pool = Pool(max_eval_concurrency)\n",
    "args = []\n",
    "poly_degrees=numpy.arange(2,degree)\n",
    "for deg in poly_degrees:\n",
    "    num_terms = int(numpy.round(comb(num_vars+deg, num_vars)))\n",
    "    basis_matrix_view = basis_matrix[:,:num_terms]\n",
    "    args.append(basis_matrix_view)\n",
    "cv_score_per_degree = numpy.asarray(pool.map(cv_solve,args))\n",
    "print 'that_took:', time.time()-t0\n",
    "\n",
    "\n",
    "%pylab inline\n",
    "from PyDakota.visualization import cross_validated_degree_bar_plot\n",
    "cross_validated_degree_bar_plot(cv_score_per_degree, poly_degrees)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
